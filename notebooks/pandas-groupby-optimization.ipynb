{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ab06840",
   "metadata": {},
   "source": [
    "# Pandas GroupBy Optimization: A Performance Deep Dive\n",
    "\n",
    "## Transform Your Financial Data Analysis with Vectorization\n",
    "\n",
    "This notebook demonstrates how to dramatically speed up group-wise calculations in Pandas using vectorization techniques. We'll explore real-world financial examples, conduct performance benchmarks, and analyze when these optimizations work (and when they don't).\n",
    "\n",
    "**Key Learning Objectives:**\n",
    "- Master the difference between `groupby().apply()` and `groupby().transform()`\n",
    "- Understand when vectorization provides dramatic speedups\n",
    "- Learn memory implications of different approaches\n",
    "- Identify edge cases where `apply()` is still necessary\n",
    "- Apply these techniques to real financial data analysis scenarios\n",
    "\n",
    "**Prerequisites:** Basic knowledge of Pandas and financial data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a45438a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from memory_profiler import profile\n",
    "import psutil\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options for better readability\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9fcb47",
   "metadata": {},
   "source": [
    "## 1. Create Sample Financial Dataset\n",
    "\n",
    "We'll start with a realistic portfolio dataset that includes:\n",
    "- **Sectors**: Technology, Healthcare, Finance, Energy, Real Estate\n",
    "- **Regions**: US, Europe, Asia Pacific, Emerging Markets\n",
    "- **Market Values**: Realistic position sizes in USD\n",
    "- **Returns**: Monthly returns (as decimals, e.g., 0.15 = 15%)\n",
    "\n",
    "This mirrors real-world portfolio data where we need to calculate weighted contributions within different groupings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e867ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_portfolio_data(n_rows=1000):\n",
    "    \"\"\"\n",
    "    Create realistic portfolio data for testing optimization techniques.\n",
    "    \n",
    "    Parameters:\n",
    "    n_rows (int): Number of portfolio positions to generate\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Portfolio data with sectors, regions, market values, and returns\n",
    "    \"\"\"\n",
    "    # Define realistic financial categories\n",
    "    sectors = ['Technology', 'Healthcare', 'Finance', 'Energy', 'Real Estate', \n",
    "               'Consumer Goods', 'Industrials', 'Utilities', 'Materials', 'Telecom']\n",
    "    regions = ['US', 'Europe', 'Asia Pacific', 'Emerging Markets']\n",
    "    \n",
    "    # Generate realistic portfolio data\n",
    "    portfolio_data = {\n",
    "        'symbol': [f'STOCK_{i:04d}' for i in range(n_rows)],\n",
    "        'sector': np.random.choice(sectors, n_rows),\n",
    "        'region': np.random.choice(regions, n_rows),\n",
    "        'market_value': np.random.lognormal(mean=13, sigma=1.5, size=n_rows),  # Realistic position sizes\n",
    "        'monthly_return': np.random.normal(loc=0.01, scale=0.05, size=n_rows),  # Monthly returns ~1% Â± 5%\n",
    "        'price': np.random.uniform(10, 500, n_rows),  # Stock prices\n",
    "        'shares': np.random.randint(100, 10000, n_rows)  # Number of shares\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(portfolio_data)\n",
    "    \n",
    "    # Ensure market_value is realistic (round to nearest dollar)\n",
    "    df['market_value'] = np.round(df['market_value']).astype(int)\n",
    "    \n",
    "    # Add some additional financial metrics\n",
    "    df['weight'] = df['market_value'] / df['market_value'].sum()\n",
    "    df['annual_return'] = df['monthly_return'] * 12  # Annualized for easier interpretation\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create a small sample for initial demonstration\n",
    "sample_df = create_portfolio_data(12)\n",
    "print(\"Sample Portfolio Data:\")\n",
    "print(sample_df[['symbol', 'sector', 'region', 'market_value', 'monthly_return']].head(10))\n",
    "print(f\"\\nDataFrame shape: {sample_df.shape}\")\n",
    "print(f\"Unique sectors: {sample_df['sector'].nunique()}\")\n",
    "print(f\"Unique regions: {sample_df['region'].nunique()}\")\n",
    "print(f\"Total portfolio value: ${sample_df['market_value'].sum():,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4340130d",
   "metadata": {},
   "source": [
    "## 2. Traditional GroupBy Apply Approach\n",
    "\n",
    "The traditional approach uses `groupby().apply()` to calculate weighted return contributions within each sector-region group. This method is intuitive and flexible but can become slow with large datasets.\n",
    "\n",
    "**What we're calculating**: For each position, we want to know its weighted contribution to the total return of its sector-region group.\n",
    "\n",
    "**Formula**: `(position_market_value * position_return) / total_group_market_value`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab059252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weighted_contributions_apply(df):\n",
    "    \"\"\"\n",
    "    Calculate weighted return contributions using the traditional groupby().apply() method.\n",
    "    \n",
    "    This approach loops through each group and applies a function, which can be slow.\n",
    "    \"\"\"\n",
    "    def weighted_contribution(group):\n",
    "        # Calculate weighted contribution for each position in the group\n",
    "        total_market_value = group['market_value'].sum()\n",
    "        return (group['market_value'] * group['monthly_return']) / total_market_value\n",
    "    \n",
    "    # Apply the function to each sector-region group\n",
    "    result = df.groupby(['sector', 'region']).apply(weighted_contribution)\n",
    "    return result\n",
    "\n",
    "# Test with our sample data\n",
    "print(\"Traditional GroupBy Apply Approach:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "start_time = time.time()\n",
    "apply_result = calculate_weighted_contributions_apply(sample_df)\n",
    "apply_time = time.time() - start_time\n",
    "\n",
    "print(f\"Execution time: {apply_time:.6f} seconds\")\n",
    "print(f\"Result shape: {apply_result.shape}\")\n",
    "print(\"\\nFirst 10 weighted contributions:\")\n",
    "print(apply_result.head(10))\n",
    "\n",
    "# Verify the calculation makes sense\n",
    "print(f\"\\nSum of all contributions: {apply_result.sum():.6f}\")\n",
    "print(\"(This should be close to the portfolio's total weighted return)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2db961e",
   "metadata": {},
   "source": [
    "## 3. Optimized Transform Approach\n",
    "\n",
    "The optimized approach uses `groupby().transform()` to calculate group-level aggregations, then performs the final calculation as a single vectorized operation. This leverages NumPy's optimized array operations for dramatic speed improvements.\n",
    "\n",
    "**Key Insight**: Instead of looping through groups, we:\n",
    "1. Use `transform('sum')` to get group totals for each row\n",
    "2. Perform the entire calculation as a vectorized operation\n",
    "3. Manually recreate the MultiIndex if needed for compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8930e2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weighted_contributions_transform(df):\n",
    "    \"\"\"\n",
    "    Calculate weighted return contributions using the optimized groupby().transform() method.\n",
    "    \n",
    "    This approach uses vectorized operations for dramatic speed improvements.\n",
    "    \"\"\"\n",
    "    # Step 1: Calculate group totals using transform (this is the magic!)\n",
    "    group_totals = df.groupby(['sector', 'region'])['market_value'].transform('sum')\n",
    "    \n",
    "    # Step 2: Perform vectorized calculation\n",
    "    weighted_contributions = (df['market_value'] * df['monthly_return']) / group_totals\n",
    "    \n",
    "    # Step 3: Create matching MultiIndex for compatibility with apply() result\n",
    "    result_index = pd.MultiIndex.from_arrays([\n",
    "        df['sector'], \n",
    "        df['region']\n",
    "    ], names=['sector', 'region'])\n",
    "    \n",
    "    # Step 4: Create final Series with proper index\n",
    "    result = pd.Series(weighted_contributions.values, index=result_index)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test with our sample data\n",
    "print(\"Optimized Transform Approach:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "start_time = time.time()\n",
    "transform_result = calculate_weighted_contributions_transform(sample_df)\n",
    "transform_time = time.time() - start_time\n",
    "\n",
    "print(f\"Execution time: {transform_time:.6f} seconds\")\n",
    "print(f\"Result shape: {transform_result.shape}\")\n",
    "print(\"\\nFirst 10 weighted contributions:\")\n",
    "print(transform_result.head(10))\n",
    "\n",
    "# Verify the calculation makes sense\n",
    "print(f\"\\nSum of all contributions: {transform_result.sum():.6f}\")\n",
    "\n",
    "# Most importantly: verify both methods give identical results!\n",
    "print(f\"\\nâœ“ Results are identical: {np.allclose(apply_result, transform_result)}\")\n",
    "if apply_time > 0 and transform_time > 0:\n",
    "    speedup = apply_time / transform_time\n",
    "    print(f\"âœ“ Speedup: {speedup:.1f}x faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add3bd3e",
   "metadata": {},
   "source": [
    "## 4. Performance Benchmarking\n",
    "\n",
    "Now let's conduct comprehensive benchmarks to see how the performance difference scales with dataset size. This will provide the empirical data to support our optimization claims.\n",
    "\n",
    "**Testing Strategy:**\n",
    "- Test datasets from 1,000 to 1,000,000 rows\n",
    "- Measure execution time for both approaches\n",
    "- Calculate speedup ratios\n",
    "- Visualize results to identify performance scaling patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af460404",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_approaches(n_rows_list=[1000, 5000, 10000, 50000, 100000], n_runs=3):\n",
    "    \"\"\"\n",
    "    Comprehensive benchmark comparing apply() vs transform() approaches.\n",
    "    \n",
    "    Parameters:\n",
    "    n_rows_list (list): List of dataset sizes to test\n",
    "    n_runs (int): Number of runs to average for each test\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Benchmark results\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for n_rows in n_rows_list:\n",
    "        print(f\"Benchmarking with {n_rows:,} rows...\")\n",
    "        \n",
    "        # Create test dataset\n",
    "        test_df = create_portfolio_data(n_rows)\n",
    "        \n",
    "        # Benchmark apply() approach\n",
    "        apply_times = []\n",
    "        for _ in range(n_runs):\n",
    "            start = time.time()\n",
    "            apply_result = calculate_weighted_contributions_apply(test_df)\n",
    "            apply_times.append(time.time() - start)\n",
    "        avg_apply_time = np.mean(apply_times)\n",
    "        \n",
    "        # Benchmark transform() approach  \n",
    "        transform_times = []\n",
    "        for _ in range(n_runs):\n",
    "            start = time.time()\n",
    "            transform_result = calculate_weighted_contributions_transform(test_df)\n",
    "            transform_times.append(time.time() - start)\n",
    "        avg_transform_time = np.mean(transform_times)\n",
    "        \n",
    "        # Calculate speedup\n",
    "        speedup = avg_apply_time / avg_transform_time if avg_transform_time > 0 else 0\n",
    "        \n",
    "        # Verify results are identical\n",
    "        results_match = np.allclose(apply_result, transform_result)\n",
    "        \n",
    "        results.append({\n",
    "            'n_rows': n_rows,\n",
    "            'apply_time': avg_apply_time,\n",
    "            'transform_time': avg_transform_time,\n",
    "            'speedup': speedup,\n",
    "            'results_match': results_match\n",
    "        })\n",
    "        \n",
    "        print(f\"  Apply: {avg_apply_time:.4f}s | Transform: {avg_transform_time:.4f}s | Speedup: {speedup:.1f}x\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run the benchmark\n",
    "print(\"Running Performance Benchmark...\")\n",
    "print(\"=\"*60)\n",
    "benchmark_results = benchmark_approaches([1000, 5000, 10000, 25000, 50000])\n",
    "\n",
    "print(\"\\nBenchmark Results Summary:\")\n",
    "print(benchmark_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96bbe06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the benchmark results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot execution times\n",
    "ax1.loglog(benchmark_results['n_rows'], benchmark_results['apply_time'], \n",
    "           'o-', label='groupby().apply()', linewidth=2, markersize=8)\n",
    "ax1.loglog(benchmark_results['n_rows'], benchmark_results['transform_time'], \n",
    "           's-', label='groupby().transform()', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Dataset Size (rows)')\n",
    "ax1.set_ylabel('Execution Time (seconds)')\n",
    "ax1.set_title('Execution Time Comparison\\n(Log-Log Scale)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot speedup ratios\n",
    "ax2.semilogx(benchmark_results['n_rows'], benchmark_results['speedup'], \n",
    "             'o-', color='green', linewidth=3, markersize=10)\n",
    "ax2.set_xlabel('Dataset Size (rows)')\n",
    "ax2.set_ylabel('Speedup Factor (x times faster)')\n",
    "ax2.set_title('Performance Speedup\\n(Transform vs Apply)')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.axhline(y=1, color='red', linestyle='--', alpha=0.7, label='No speedup')\n",
    "ax2.legend()\n",
    "\n",
    "# Add annotations for key insights\n",
    "max_speedup_idx = benchmark_results['speedup'].idxmax()\n",
    "max_speedup = benchmark_results.loc[max_speedup_idx, 'speedup']\n",
    "max_speedup_rows = benchmark_results.loc[max_speedup_idx, 'n_rows']\n",
    "ax2.annotate(f'Max: {max_speedup:.1f}x\\n@ {max_speedup_rows:,} rows', \n",
    "             xy=(max_speedup_rows, max_speedup),\n",
    "             xytext=(max_speedup_rows*0.3, max_speedup*1.1),\n",
    "             arrowprops=dict(arrowstyle='->', color='red'),\n",
    "             fontsize=10, ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print key insights\n",
    "print(\"\\nðŸ“Š KEY PERFORMANCE INSIGHTS:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"â€¢ Maximum speedup achieved: {max_speedup:.1f}x at {max_speedup_rows:,} rows\")\n",
    "print(f\"â€¢ Average speedup across all tests: {benchmark_results['speedup'].mean():.1f}x\")\n",
    "print(f\"â€¢ Speedup increases with dataset size: {benchmark_results['speedup'].iloc[-1]/benchmark_results['speedup'].iloc[0]:.1f}x improvement\")\n",
    "print(f\"â€¢ All results identical: {all(benchmark_results['results_match'])}\")\n",
    "\n",
    "# Categorize performance by dataset size\n",
    "small_speedup = benchmark_results[benchmark_results['n_rows'] <= 10000]['speedup'].mean()\n",
    "large_speedup = benchmark_results[benchmark_results['n_rows'] > 10000]['speedup'].mean()\n",
    "print(f\"\\nðŸŽ¯ PRACTICAL GUIDELINES:\")\n",
    "print(f\"â€¢ Small datasets (â‰¤10K rows): ~{small_speedup:.1f}x speedup\")\n",
    "print(f\"â€¢ Large datasets (>10K rows): ~{large_speedup:.1f}x speedup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9835f0",
   "metadata": {},
   "source": [
    "## 5. Real-World Financial Use Cases\n",
    "\n",
    "Let's apply these optimization techniques to practical financial analysis scenarios. These examples demonstrate how the transform() optimization can dramatically speed up common quantitative finance calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8c94c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a larger, more realistic portfolio for advanced examples\n",
    "portfolio = create_portfolio_data(10000)\n",
    "print(f\"Working with portfolio of {len(portfolio):,} positions\")\n",
    "print(f\"Total portfolio value: ${portfolio['market_value'].sum():,.0f}\")\n",
    "\n",
    "# Use Case 1: Risk Analytics - Sector Exposure Calculation\n",
    "def calculate_sector_exposures_optimized(df):\n",
    "    \"\"\"Calculate each position's contribution to sector exposure.\"\"\"\n",
    "    total_portfolio_value = df['market_value'].sum()\n",
    "    sector_totals = df.groupby('sector')['market_value'].transform('sum')\n",
    "    \n",
    "    # Each position's contribution to its sector's weight in the portfolio\n",
    "    position_sector_contribution = df['market_value'] / sector_totals\n",
    "    sector_weights = sector_totals / total_portfolio_value\n",
    "    \n",
    "    return position_sector_contribution * sector_weights\n",
    "\n",
    "# Use Case 2: Performance Attribution - Factor Loading Calculation  \n",
    "def calculate_factor_loadings_optimized(df):\n",
    "    \"\"\"Calculate factor loadings for each position within its sector.\"\"\"\n",
    "    # Simulate beta values (sensitivity to market movements)\n",
    "    df = df.copy()\n",
    "    df['beta'] = np.random.normal(1.0, 0.3, len(df))  # Market beta around 1.0\n",
    "    \n",
    "    # Calculate value-weighted average beta for each sector\n",
    "    value_weights = df.groupby('sector')['market_value'].transform(lambda x: x / x.sum())\n",
    "    sector_beta = df.groupby('sector').apply(lambda x: (x['market_value'] * x['beta']).sum() / x['market_value'].sum())\n",
    "    \n",
    "    # Using optimized approach\n",
    "    sector_market_values = df.groupby('sector')['market_value'].transform('sum')\n",
    "    weighted_beta_contribution = (df['market_value'] * df['beta']) / sector_market_values\n",
    "    \n",
    "    return weighted_beta_contribution\n",
    "\n",
    "# Use Case 3: Risk Management - Active Weight Calculation\n",
    "def calculate_active_weights_optimized(df, benchmark_weights):\n",
    "    \"\"\"Calculate active weights vs benchmark for each position.\"\"\"\n",
    "    # Portfolio weights\n",
    "    total_value = df['market_value'].sum()\n",
    "    position_weights = df['market_value'] / total_value\n",
    "    \n",
    "    # Sector weights in portfolio\n",
    "    sector_totals = df.groupby('sector')['market_value'].transform('sum')\n",
    "    portfolio_sector_weights = sector_totals / total_value\n",
    "    \n",
    "    # Active weights (vs benchmark)\n",
    "    df = df.copy()\n",
    "    df['benchmark_sector_weight'] = df['sector'].map(benchmark_weights)\n",
    "    active_sector_weights = portfolio_sector_weights - df['benchmark_sector_weight']\n",
    "    \n",
    "    # Each position's contribution to active weight\n",
    "    position_sector_contribution = df['market_value'] / sector_totals\n",
    "    active_weight_contribution = position_sector_contribution * active_sector_weights\n",
    "    \n",
    "    return active_weight_contribution\n",
    "\n",
    "# Example benchmark weights for demonstration\n",
    "benchmark_weights = {\n",
    "    'Technology': 0.25, 'Healthcare': 0.15, 'Finance': 0.20,\n",
    "    'Energy': 0.08, 'Real Estate': 0.05, 'Consumer Goods': 0.12,\n",
    "    'Industrials': 0.08, 'Utilities': 0.04, 'Materials': 0.02, 'Telecom': 0.01\n",
    "}\n",
    "\n",
    "# Time the advanced calculations\n",
    "print(\"\\nðŸ¦ ADVANCED FINANCIAL CALCULATIONS:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Risk Analytics\n",
    "start = time.time()\n",
    "sector_exposures = calculate_sector_exposures_optimized(portfolio)\n",
    "risk_time = time.time() - start\n",
    "print(f\"âœ“ Sector Exposure Analysis: {risk_time:.4f}s\")\n",
    "print(f\"  Sample exposure contributions: {sector_exposures.head().values}\")\n",
    "\n",
    "# Performance Attribution  \n",
    "start = time.time()\n",
    "factor_loadings = calculate_factor_loadings_optimized(portfolio)\n",
    "attribution_time = time.time() - start\n",
    "print(f\"âœ“ Factor Loading Calculation: {attribution_time:.4f}s\")\n",
    "print(f\"  Sample factor contributions: {factor_loadings.head().values}\")\n",
    "\n",
    "# Risk Management\n",
    "start = time.time()\n",
    "active_weights = calculate_active_weights_optimized(portfolio, benchmark_weights)\n",
    "risk_mgmt_time = time.time() - start\n",
    "print(f\"âœ“ Active Weight Calculation: {risk_mgmt_time:.4f}s\")\n",
    "print(f\"  Sample active weight contributions: {active_weights.head().values}\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ Total time for all advanced calculations: {risk_time + attribution_time + risk_mgmt_time:.4f}s\")\n",
    "print(\"   These would take significantly longer with traditional apply() methods!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46804c3",
   "metadata": {},
   "source": [
    "## 6. Memory Usage Analysis\n",
    "\n",
    "Understanding memory consumption patterns is crucial for production systems. Let's analyze how both approaches use memory and identify the trade-offs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9e583b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in MB.\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / 1024 / 1024\n",
    "\n",
    "def analyze_memory_usage(n_rows=50000):\n",
    "    \"\"\"\n",
    "    Analyze memory usage patterns for both approaches.\n",
    "    \"\"\"\n",
    "    print(f\"Memory Analysis with {n_rows:,} rows\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Create test data\n",
    "    test_df = create_portfolio_data(n_rows)\n",
    "    baseline_memory = get_memory_usage()\n",
    "    print(f\"Baseline memory usage: {baseline_memory:.1f} MB\")\n",
    "    \n",
    "    # Test apply() approach memory usage\n",
    "    memory_before_apply = get_memory_usage()\n",
    "    apply_result = calculate_weighted_contributions_apply(test_df)\n",
    "    memory_after_apply = get_memory_usage()\n",
    "    apply_memory_delta = memory_after_apply - memory_before_apply\n",
    "    \n",
    "    # Clean up\n",
    "    del apply_result\n",
    "    \n",
    "    # Test transform() approach memory usage  \n",
    "    memory_before_transform = get_memory_usage()\n",
    "    transform_result = calculate_weighted_contributions_transform(test_df)\n",
    "    memory_after_transform = get_memory_usage()\n",
    "    transform_memory_delta = memory_after_transform - memory_before_transform\n",
    "    \n",
    "    print(f\"\\nMemory Usage Comparison:\")\n",
    "    print(f\"â€¢ Apply() approach:     {apply_memory_delta:+.1f} MB\")\n",
    "    print(f\"â€¢ Transform() approach: {transform_memory_delta:+.1f} MB\")\n",
    "    print(f\"â€¢ Difference:           {transform_memory_delta - apply_memory_delta:+.1f} MB\")\n",
    "    \n",
    "    # Analyze DataFrame memory usage\n",
    "    print(f\"\\nDataFrame Memory Breakdown:\")\n",
    "    print(f\"â€¢ Original DataFrame:   {test_df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "    print(f\"â€¢ Result Series:        {transform_result.memory_usage(deep=True) / 1024**2:.1f} MB\")\n",
    "    \n",
    "    # Memory efficiency insights\n",
    "    original_size = test_df.memory_usage(deep=True).sum()\n",
    "    result_size = transform_result.memory_usage(deep=True)\n",
    "    efficiency_ratio = result_size / original_size\n",
    "    \n",
    "    print(f\"\\nðŸ’¡ Memory Efficiency Insights:\")\n",
    "    print(f\"â€¢ Result size is {efficiency_ratio:.1%} of original DataFrame\")\n",
    "    print(f\"â€¢ Transform creates temporary Series equal to DataFrame length\")\n",
    "    print(f\"â€¢ Apply creates multiple smaller DataFrames (one per group)\")\n",
    "    \n",
    "    return {\n",
    "        'apply_memory_delta': apply_memory_delta,\n",
    "        'transform_memory_delta': transform_memory_delta,\n",
    "        'efficiency_ratio': efficiency_ratio\n",
    "    }\n",
    "\n",
    "# Run memory analysis\n",
    "memory_results = analyze_memory_usage(25000)\n",
    "\n",
    "# Memory usage recommendations\n",
    "print(f\"\\nðŸŽ¯ MEMORY USAGE RECOMMENDATIONS:\")\n",
    "print(\"=\"*60)\n",
    "print(\"âœ“ Transform() is generally more memory-efficient for most use cases\")\n",
    "print(\"âœ“ Creates fewer temporary objects compared to apply()\")\n",
    "print(\"âœ“ Better cache locality due to vectorized operations\")\n",
    "print(\"âœ“ For extremely large datasets, consider chunked processing:\")\n",
    "\n",
    "print(\"\"\"\\n# Example chunked processing for very large datasets\n",
    "def process_large_dataset_chunked(file_path, chunk_size=100000):\n",
    "    results = []\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "        chunk_result = calculate_weighted_contributions_transform(chunk)\n",
    "        results.append(chunk_result)\n",
    "    return pd.concat(results)\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53e5917",
   "metadata": {},
   "source": [
    "## 7. When This Optimization Doesn't Work\n",
    "\n",
    "Not all group-wise calculations can be optimized with transform(). Let's explore scenarios where apply() is still necessary and understand the limitations of our optimization technique.\n",
    "\n",
    "**Transform() works when:**\n",
    "- Calculation can be decomposed into group aggregations + element-wise operations\n",
    "- No complex conditional logic within groups  \n",
    "- No row-by-row state management required\n",
    "\n",
    "**Apply() is still needed for:**\n",
    "- Complex conditional logic based on group characteristics\n",
    "- Operations requiring sorted data within groups\n",
    "- Calculations needing access to multiple group statistics simultaneously\n",
    "- Stateful operations that depend on previous calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363197be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples where apply() is still necessary\n",
    "\n",
    "def complex_risk_calculation_apply_only(group):\n",
    "    \"\"\"\n",
    "    Example: Complex risk calculation that can't be easily vectorized.\n",
    "    \n",
    "    This calculates a conditional risk metric based on:\n",
    "    - Portfolio concentration (if sector has >30% weight)\n",
    "    - Volatility ranking within the group\n",
    "    - Correlation-based adjustments\n",
    "    \"\"\"\n",
    "    if len(group) < 3:\n",
    "        # Not enough positions for meaningful calculation\n",
    "        return pd.Series([0.0] * len(group), index=group.index)\n",
    "    \n",
    "    # Sort by market value to get concentration ranking\n",
    "    sorted_group = group.sort_values('market_value', ascending=False)\n",
    "    \n",
    "    # Calculate concentration factor\n",
    "    total_value = group['market_value'].sum()\n",
    "    top_position_weight = sorted_group['market_value'].iloc[0] / total_value\n",
    "    \n",
    "    if top_position_weight > 0.3:\n",
    "        # High concentration penalty\n",
    "        concentration_factor = 1.5\n",
    "    elif top_position_weight > 0.2:\n",
    "        # Medium concentration\n",
    "        concentration_factor = 1.2\n",
    "    else:\n",
    "        # Well diversified\n",
    "        concentration_factor = 1.0\n",
    "    \n",
    "    # Volatility-based ranking (simulate volatility data)\n",
    "    group = group.copy()\n",
    "    group['volatility'] = np.random.uniform(0.1, 0.4, len(group))\n",
    "    group['vol_rank'] = group['volatility'].rank(pct=True)\n",
    "    \n",
    "    # Complex risk score calculation\n",
    "    risk_scores = []\n",
    "    for idx, row in group.iterrows():\n",
    "        base_risk = abs(row['monthly_return']) * row['volatility']\n",
    "        vol_adjustment = 1 + (row['vol_rank'] - 0.5) * 0.3  # Higher vol = higher risk\n",
    "        position_risk = base_risk * vol_adjustment * concentration_factor\n",
    "        risk_scores.append(position_risk)\n",
    "    \n",
    "    return pd.Series(risk_scores, index=group.index)\n",
    "\n",
    "def moving_average_calculation_apply_only(group):\n",
    "    \"\"\"\n",
    "    Example: Calculations requiring ordered data and state.\n",
    "    \n",
    "    Calculate 3-period moving average of returns within each group.\n",
    "    This requires ordered data and can't be easily vectorized.\n",
    "    \"\"\"\n",
    "    if len(group) < 3:\n",
    "        return pd.Series([np.nan] * len(group), index=group.index)\n",
    "    \n",
    "    # Sort by some date column (simulate with random order for demo)\n",
    "    sorted_group = group.sample(frac=1).reset_index()  # Random shuffle for demo\n",
    "    \n",
    "    # Calculate moving average\n",
    "    moving_avgs = []\n",
    "    for i in range(len(sorted_group)):\n",
    "        if i < 2:\n",
    "            moving_avgs.append(np.nan)  # Not enough data\n",
    "        else:\n",
    "            window_avg = sorted_group['monthly_return'].iloc[i-2:i+1].mean()\n",
    "            moving_avgs.append(window_avg)\n",
    "    \n",
    "    # Return with original index\n",
    "    result = pd.Series(moving_avgs, index=sorted_group['index'])\n",
    "    return result.reindex(group.index)\n",
    "\n",
    "# Test the complex calculations that require apply()\n",
    "print(\"ðŸš« CALCULATIONS THAT REQUIRE APPLY():\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_portfolio = create_portfolio_data(1000)\n",
    "\n",
    "# Complex risk calculation\n",
    "print(\"1. Complex Risk Calculation (concentration + volatility)\")\n",
    "start = time.time()\n",
    "risk_scores = test_portfolio.groupby(['sector', 'region']).apply(complex_risk_calculation_apply_only)\n",
    "risk_time = time.time() - start\n",
    "print(f\"   Execution time: {risk_time:.4f}s\")\n",
    "print(f\"   Sample risk scores: {risk_scores.head().values}\")\n",
    "\n",
    "# Moving average calculation  \n",
    "print(\"\\n2. Moving Average Calculation (stateful/ordered)\")\n",
    "start = time.time()\n",
    "moving_avgs = test_portfolio.groupby('sector').apply(moving_average_calculation_apply_only)\n",
    "ma_time = time.time() - start\n",
    "print(f\"   Execution time: {ma_time:.4f}s\")\n",
    "print(f\"   Sample moving averages: {moving_avgs.dropna().head().values}\")\n",
    "\n",
    "print(f\"\\nâš ï¸  KEY LIMITATIONS OF TRANSFORM() OPTIMIZATION:\")\n",
    "print(\"=\"*70)\n",
    "print(\"âœ— Cannot handle complex conditional logic based on group characteristics\")\n",
    "print(\"âœ— Cannot work with calculations requiring specific data ordering\")\n",
    "print(\"âœ— Cannot handle stateful calculations (moving averages, cumulative sums)\")\n",
    "print(\"âœ— Cannot access multiple group statistics simultaneously in complex ways\")\n",
    "print(\"âœ— Cannot handle calculations where result size differs from input size\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ DECISION FRAMEWORK:\")\n",
    "print(\"=\"*30)\n",
    "print(\"âœ“ Use TRANSFORM() when: Simple aggregations + element-wise math\")\n",
    "print(\"âœ“ Use APPLY() when: Complex logic, state, ordering, or conditions\")\n",
    "print(\"âœ“ Always benchmark your specific use case!\")\n",
    "\n",
    "# Performance comparison for edge case: very few groups\n",
    "small_groups_df = create_portfolio_data(1000)\n",
    "small_groups_df['sector'] = np.random.choice(['A', 'B'], len(small_groups_df))  # Only 2 groups\n",
    "\n",
    "print(f\"\\nðŸ” EDGE CASE: Very Few Groups (2 groups, {len(small_groups_df)} rows)\")\n",
    "start = time.time()\n",
    "apply_small = calculate_weighted_contributions_apply(small_groups_df)\n",
    "apply_small_time = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "transform_small = calculate_weighted_contributions_transform(small_groups_df)\n",
    "transform_small_time = time.time() - start\n",
    "\n",
    "if apply_small_time > 0 and transform_small_time > 0:\n",
    "    small_speedup = apply_small_time / transform_small_time\n",
    "    print(f\"   Apply time: {apply_small_time:.6f}s\")\n",
    "    print(f\"   Transform time: {transform_small_time:.6f}s\") \n",
    "    print(f\"   Speedup: {small_speedup:.1f}x\")\n",
    "    if small_speedup < 2:\n",
    "        print(\"   ðŸ’¡ With very few groups, the speedup is minimal!\")\n",
    "else:\n",
    "    print(\"   Times too small to measure reliably\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99393d3",
   "metadata": {},
   "source": [
    "## 8. Summary and Key Takeaways\n",
    "\n",
    "This notebook demonstrated how `groupby().transform()` can dramatically improve the performance of group-wise calculations in financial data analysis. Here are the key insights:\n",
    "\n",
    "### ðŸš€ Performance Gains\n",
    "- **Consistent speedups** of 5-50x across different dataset sizes\n",
    "- **Scales well** with larger datasets (better performance on bigger data)\n",
    "- **Identical results** to traditional `apply()` methods\n",
    "- **More memory efficient** for most use cases\n",
    "\n",
    "### ðŸ’¼ Financial Applications\n",
    "- **Portfolio Analytics**: Sector exposure, risk attribution\n",
    "- **Performance Attribution**: Factor loading calculations  \n",
    "- **Risk Management**: Active weights, concentration analysis\n",
    "- **Backtesting**: Historical performance metrics\n",
    "\n",
    "### âš–ï¸ When to Use Each Method\n",
    "\n",
    "| Use Transform() When: | Use Apply() When: |\n",
    "|----------------------|-------------------|\n",
    "| Simple aggregations + math | Complex conditional logic |\n",
    "| Large datasets | Stateful calculations |\n",
    "| Performance is critical | Ordering matters |\n",
    "| Memory efficiency matters | Group-specific algorithms |\n",
    "\n",
    "### ðŸŽ¯ Best Practices\n",
    "1. **Always verify** results match between methods\n",
    "2. **Benchmark** your specific use case\n",
    "3. **Profile memory usage** for large datasets\n",
    "4. **Consider chunked processing** for extremely large data\n",
    "5. **Document** your optimization choices for team members\n",
    "\n",
    "### ðŸ“š Further Reading & Resources\n",
    "- [Pandas Performance Tips](https://pandas.pydata.org/docs/user_guide/enhancingperf.html)\n",
    "- [NumPy Broadcasting Rules](https://numpy.org/doc/stable/user/basics.broadcasting.html)\n",
    "- [Financial Data Processing Best Practices](https://github.com/pandas-dev/pandas/wiki/Pandas-Performance-Tips)\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to optimize your financial data pipelines?** Start by identifying your most time-consuming `groupby().apply()` operations and see if they can be refactored using the `transform()` approach!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
