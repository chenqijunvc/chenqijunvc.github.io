{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ab06840",
   "metadata": {},
   "source": [
    "# Pandas GroupBy Optimization: A Performance Deep Dive\n",
    "\n",
    "## Transform Your Financial Data Analysis with Vectorization\n",
    "\n",
    "This notebook demonstrates how to dramatically speed up group-wise calculations in Pandas using vectorization techniques. We'll explore real-world financial examples, conduct performance benchmarks, and analyze when these optimizations work (and when they don't).\n",
    "\n",
    "**Key Learning Objectives:**\n",
    "- Master the difference between `groupby().apply()` and `groupby().transform()`\n",
    "- Understand when vectorization provides dramatic speedups\n",
    "- Learn memory implications of different approaches\n",
    "- Identify edge cases where `apply()` is still necessary\n",
    "- Apply these techniques to real financial data analysis scenarios\n",
    "\n",
    "**Prerequisites:** Basic knowledge of Pandas and financial data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a45438a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from memory_profiler import profile\n",
    "import psutil\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options for better readability\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9fcb47",
   "metadata": {},
   "source": [
    "## 1. Create Sample Financial Dataset\n",
    "\n",
    "We'll start with a realistic portfolio dataset that includes:\n",
    "- **Sectors**: Technology, Healthcare, Finance, Energy, Real Estate\n",
    "- **Regions**: US, Europe, Asia Pacific, Emerging Markets\n",
    "- **Market Values**: Realistic position sizes in USD\n",
    "- **Returns**: Monthly returns (as decimals, e.g., 0.15 = 15%)\n",
    "\n",
    "This mirrors real-world portfolio data where we need to calculate weighted contributions within different groupings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e867ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_portfolio_data(n_rows=1000):\n",
    "    \"\"\"\n",
    "    Create realistic portfolio data for testing optimization techniques.\n",
    "    \n",
    "    Parameters:\n",
    "    n_rows (int): Number of portfolio positions to generate\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Portfolio data with sectors, regions, market values, and returns\n",
    "    \"\"\"\n",
    "    # Define realistic financial categories\n",
    "    sectors = ['Technology', 'Healthcare', 'Finance', 'Energy', 'Real Estate', \n",
    "               'Consumer Goods', 'Industrials', 'Utilities', 'Materials', 'Telecom']\n",
    "    regions = ['US', 'Europe', 'Asia Pacific', 'Emerging Markets']\n",
    "    \n",
    "    # Generate realistic portfolio data\n",
    "    portfolio_data = {\n",
    "        'symbol': [f'STOCK_{i:04d}' for i in range(n_rows)],\n",
    "        'sector': np.random.choice(sectors, n_rows),\n",
    "        'region': np.random.choice(regions, n_rows),\n",
    "        'market_value': np.random.lognormal(mean=13, sigma=1.5, size=n_rows),  # Realistic position sizes\n",
    "        'monthly_return': np.random.normal(loc=0.01, scale=0.05, size=n_rows),  # Monthly returns ~1% Â± 5%\n",
    "        'price': np.random.uniform(10, 500, n_rows),  # Stock prices\n",
    "        'shares': np.random.randint(100, 10000, n_rows)  # Number of shares\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(portfolio_data)\n",
    "    \n",
    "    # Ensure market_value is realistic (round to nearest dollar)\n",
    "    df['market_value'] = np.round(df['market_value']).astype(int)\n",
    "    \n",
    "    # Add some additional financial metrics\n",
    "    df['weight'] = df['market_value'] / df['market_value'].sum()\n",
    "    df['annual_return'] = df['monthly_return'] * 12  # Annualized for easier interpretation\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create a small sample for initial demonstration\n",
    "sample_df = create_portfolio_data(12)\n",
    "print(\"Sample Portfolio Data:\")\n",
    "print(sample_df[['symbol', 'sector', 'region', 'market_value', 'monthly_return']].head(10))\n",
    "print(f\"\\nDataFrame shape: {sample_df.shape}\")\n",
    "print(f\"Unique sectors: {sample_df['sector'].nunique()}\")\n",
    "print(f\"Unique regions: {sample_df['region'].nunique()}\")\n",
    "print(f\"Total portfolio value: ${sample_df['market_value'].sum():,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4340130d",
   "metadata": {},
   "source": [
    "## 2. Traditional GroupBy Apply Approach\n",
    "\n",
    "The traditional approach uses `groupby().apply()` to calculate weighted return contributions within each sector-region group. This method is intuitive and flexible but can become slow with large datasets.\n",
    "\n",
    "**What we're calculating**: For each position, we want to know its weighted contribution to the total return of its sector-region group.\n",
    "\n",
    "**Formula**: `(position_market_value * position_return) / total_group_market_value`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab059252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weighted_contributions_apply(df):\n",
    "    \"\"\"\n",
    "    Calculate weighted return contributions using the traditional groupby().apply() method.\n",
    "    \n",
    "    This approach loops through each group and applies a function, which can be slow.\n",
    "    \"\"\"\n",
    "    def weighted_contribution(group):\n",
    "        # Calculate weighted contribution for each position in the group\n",
    "        total_market_value = group['market_value'].sum()\n",
    "        return (group['market_value'] * group['monthly_return']) / total_market_value\n",
    "    \n",
    "    # Apply the function to each sector-region group\n",
    "    result = df.groupby(['sector', 'region']).apply(weighted_contribution)\n",
    "    return result\n",
    "\n",
    "# Test with our sample data\n",
    "print(\"Traditional GroupBy Apply Approach:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "start_time = time.time()\n",
    "apply_result = calculate_weighted_contributions_apply(sample_df)\n",
    "apply_time = time.time() - start_time\n",
    "\n",
    "print(f\"Execution time: {apply_time:.6f} seconds\")\n",
    "print(f\"Result shape: {apply_result.shape}\")\n",
    "print(\"\\nFirst 10 weighted contributions:\")\n",
    "print(apply_result.head(10))\n",
    "\n",
    "# Verify the calculation makes sense\n",
    "print(f\"\\nSum of all contributions: {apply_result.sum():.6f}\")\n",
    "print(\"(This should be close to the portfolio's total weighted return)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2db961e",
   "metadata": {},
   "source": [
    "## 3. Optimized Transform Approach\n",
    "\n",
    "The optimized approach uses `groupby().transform()` to calculate group-level aggregations, then performs the final calculation as a single vectorized operation. This leverages NumPy's optimized array operations for dramatic speed improvements.\n",
    "\n",
    "**Key Insight**: Instead of looping through groups, we:\n",
    "1. Use `transform('sum')` to get group totals for each row\n",
    "2. Perform the entire calculation as a vectorized operation\n",
    "3. Manually recreate the MultiIndex if needed for compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8930e2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weighted_contributions_transform(df):\n",
    "    \"\"\"\n",
    "    Calculate weighted return contributions using the optimized groupby().transform() method.\n",
    "    \n",
    "    This approach uses vectorized operations for dramatic speed improvements.\n",
    "    \"\"\"\n",
    "    # Step 1: Calculate group totals using transform (this is the magic!)\n",
    "    group_totals = df.groupby(['sector', 'region'])['market_value'].transform('sum')\n",
    "    \n",
    "    # Step 2: Perform vectorized calculation\n",
    "    weighted_contributions = (df['market_value'] * df['monthly_return']) / group_totals\n",
    "    \n",
    "    # Step 3: Create matching MultiIndex for compatibility with apply() result\n",
    "    result_index = pd.MultiIndex.from_arrays([\n",
    "        df['sector'], \n",
    "        df['region']\n",
    "    ], names=['sector', 'region'])\n",
    "    \n",
    "    # Step 4: Create final Series with proper index\n",
    "    result = pd.Series(weighted_contributions.values, index=result_index)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test with our sample data\n",
    "print(\"Optimized Transform Approach:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "start_time = time.time()\n",
    "transform_result = calculate_weighted_contributions_transform(sample_df)\n",
    "transform_time = time.time() - start_time\n",
    "\n",
    "print(f\"Execution time: {transform_time:.6f} seconds\")\n",
    "print(f\"Result shape: {transform_result.shape}\")\n",
    "print(\"\\nFirst 10 weighted contributions:\")\n",
    "print(transform_result.head(10))\n",
    "\n",
    "# Verify the calculation makes sense\n",
    "print(f\"\\nSum of all contributions: {transform_result.sum():.6f}\")\n",
    "\n",
    "# Most importantly: verify both methods give identical results!\n",
    "print(f\"\\nâ Results are identical: {np.allclose(apply_result, transform_result)}\")\n",
    "if apply_time > 0 and transform_time > 0:\n",
    "    speedup = apply_time / transform_time\n",
    "    print(f\"â Speedup: {speedup:.1f}x faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add3bd3e",
   "metadata": {},
   "source": [
    "## 4. Performance Benchmarking\n",
    "\n",
    "Now let's conduct comprehensive benchmarks to see how the performance difference scales with dataset size. This will provide the empirical data to support our optimization claims.\n",
    "\n",
    "**Testing Strategy:**\n",
    "- Test datasets from 1,000 to 1,000,000 rows\n",
    "- Measure execution time for both approaches\n",
    "- Calculate speedup ratios\n",
    "- Visualize results to identify performance scaling patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af460404",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_approaches(n_rows_list=[1000, 5000, 10000, 50000, 100000], n_runs=3):\n",
    "    \"\"\"\n",
    "    Comprehensive benchmark comparing apply() vs transform() approaches.\n",
    "    \n",
    "    Parameters:\n",
    "    n_rows_list (list): List of dataset sizes to test\n",
    "    n_runs (int): Number of runs to average for each test\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Benchmark results\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for n_rows in n_rows_list:\n",
    "        print(f\"Benchmarking with {n_rows:,} rows...\")\n",
    "        \n",
    "        # Create test dataset\n",
    "        test_df = create_portfolio_data(n_rows)\n",
    "        \n",
    "        # Benchmark apply() approach\n",
    "        apply_times = []\n",
    "        for _ in range(n_runs):\n",
    "            start = time.time()\n",
    "            apply_result = calculate_weighted_contributions_apply(test_df)\n",
    "            apply_times.append(time.time() - start)\n",
    "        avg_apply_time = np.mean(apply_times)\n",
    "        \n",
    "        # Benchmark transform() approach  \n",
    "        transform_times = []\n",
    "        for _ in range(n_runs):\n",
    "            start = time.time()\n",
    "            transform_result = calculate_weighted_contributions_transform(test_df)\n",
    "            transform_times.append(time.time() - start)\n",
    "        avg_transform_time = np.mean(transform_times)\n",
    "        \n",
    "        # Calculate speedup\n",
    "        speedup = avg_apply_time / avg_transform_time if avg_transform_time > 0 else 0\n",
    "        \n",
    "        # Verify results are identical\n",
    "        results_match = np.allclose(apply_result, transform_result)\n",
    "        \n",
    "        results.append({\n",
    "            'n_rows': n_rows,\n",
    "            'apply_time': avg_apply_time,\n",
    "            'transform_time': avg_transform_time,\n",
    "            'speedup': speedup,\n",
    "            'results_match': results_match\n",
    "        })\n",
    "        \n",
    "        print(f\"  Apply: {avg_apply_time:.4f}s | Transform: {avg_transform_time:.4f}s | Speedup: {speedup:.1f}x\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run the benchmark\n",
    "print(\"Running Performance Benchmark...\")\n",
    "print(\"=\"*60)\n",
    "benchmark_results = benchmark_approaches([1000, 5000, 10000, 25000, 50000])\n",
    "\n",
    "print(\"\\nBenchmark Results Summary:\")\n",
    "print(benchmark_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96bbe06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the benchmark results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot execution times\n",
    "ax1.loglog(benchmark_results['n_rows'], benchmark_results['apply_time'], \n",
    "           'o-', label='groupby().apply()', linewidth=2, markersize=8)\n",
    "ax1.loglog(benchmark_results['n_rows'], benchmark_results['transform_time'], \n",
    "           's-', label='groupby().transform()', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Dataset Size (rows)')\n",
    "ax1.set_ylabel('Execution Time (seconds)')\n",
    "ax1.set_title('Execution Time Comparison\\n(Log-Log Scale)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot speedup ratios\n",
    "ax2.semilogx(benchmark_results['n_rows'], benchmark_results['speedup'], \n",
    "             'o-', color='green', linewidth=3, markersize=10)\n",
    "ax2.set_xlabel('Dataset Size (rows)')\n",
    "ax2.set_ylabel('Speedup Factor (x times faster)')\n",
    "ax2.set_title('Performance Speedup\\n(Transform vs Apply)')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.axhline(y=1, color='red', linestyle='--', alpha=0.7, label='No speedup')\n",
    "ax2.legend()\n",
    "\n",
    "# Add annotations for key insights\n",
    "max_speedup_idx = benchmark_results['speedup'].idxmax()\n",
    "max_speedup = benchmark_results.loc[max_speedup_idx, 'speedup']\n",
    "max_speedup_rows = benchmark_results.loc[max_speedup_idx, 'n_rows']\n",
    "ax2.annotate(f'Max: {max_speedup:.1f}x\\n@ {max_speedup_rows:,} rows', \n",
    "             xy=(max_speedup_rows, max_speedup),\n",
    "             xytext=(max_speedup_rows*0.3, max_speedup*1.1),\n",
    "             arrowprops=dict(arrowstyle='->', color='red'),\n",
    "             fontsize=10, ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print key insights\n",
    "print(\"\\nð KEY PERFORMANCE INSIGHTS:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"â¢ Maximum speedup achieved: {max_speedup:.1f}x at {max_speedup_rows:,} rows\")\n",
    "print(f\"â¢ Average speedup across all tests: {benchmark_results['speedup'].mean():.1f}x\")\n",
    "print(f\"â¢ Speedup increases with dataset size: {benchmark_results['speedup'].iloc[-1]/benchmark_results['speedup'].iloc[0]:.1f}x improvement\")\n",
    "print(f\"â¢ All results identical: {all(benchmark_results['results_match'])}\")\n",
    "\n",
    "# Categorize performance by dataset size\n",
    "small_speedup = benchmark_results[benchmark_results['n_rows'] <= 10000]['speedup'].mean()\n",
    "large_speedup = benchmark_results[benchmark_results['n_rows'] > 10000]['speedup'].mean()\n",
    "print(f\"\\nð¯ PRACTICAL GUIDELINES:\")\n",
    "print(f\"â¢ Small datasets (â¤10K rows): ~{small_speedup:.1f}x speedup\")\n",
    "print(f\"â¢ Large datasets (>10K rows): ~{large_speedup:.1f}x speedup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9835f0",
   "metadata": {},
   "source": [
    "## 5. Real-World Financial Use Cases\n",
    "\n",
    "Let's apply these optimization techniques to practical financial analysis scenarios. These examples demonstrate how the transform() optimization can dramatically speed up common quantitative finance calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8c94c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a larger, more realistic portfolio for advanced examples\n",
    "portfolio = create_portfolio_data(10000)\n",
    "print(f\"Working with portfolio of {len(portfolio):,} positions\")\n",
    "print(f\"Total portfolio value: ${portfolio['market_value'].sum():,.0f}\")\n",
    "\n",
    "# Use Case 1: Risk Analytics - Sector Exposure Calculation\n",
    "def calculate_sector_exposures_optimized(df):\n",
    "    \"\"\"Calculate each position's contribution to sector exposure.\"\"\"\n",
    "    total_portfolio_value = df['market_value'].sum()\n",
    "    sector_totals = df.groupby('sector')['market_value'].transform('sum')\n",
    "    \n",
    "    # Each position's contribution to its sector's weight in the portfolio\n",
    "    position_sector_contribution = df['market_value'] / sector_totals\n",
    "    sector_weights = sector_totals / total_portfolio_value\n",
    "    \n",
    "    return position_sector_contribution * sector_weights\n",
    "\n",
    "# Use Case 2: Performance Attribution - Factor Loading Calculation  \n",
    "def calculate_factor_loadings_optimized(df):\n",
    "    \"\"\"Calculate factor loadings for each position within its sector.\"\"\"\n",
    "    # Simulate beta values (sensitivity to market movements)\n",
    "    df = df.copy()\n",
    "    df['beta'] = np.random.normal(1.0, 0.3, len(df))  # Market beta around 1.0\n",
    "    \n",
    "    # Calculate value-weighted average beta for each sector\n",
    "    value_weights = df.groupby('sector')['market_value'].transform(lambda x: x / x.sum())\n",
    "    sector_beta = df.groupby('sector').apply(lambda x: (x['market_value'] * x['beta']).sum() / x['market_value'].sum())\n",
    "    \n",
    "    # Using optimized approach\n",
    "    sector_market_values = df.groupby('sector')['market_value'].transform('sum')\n",
    "    weighted_beta_contribution = (df['market_value'] * df['beta']) / sector_market_values\n",
    "    \n",
    "    return weighted_beta_contribution\n",
    "\n",
    "# Use Case 3: Risk Management - Active Weight Calculation\n",
    "def calculate_active_weights_optimized(df, benchmark_weights):\n",
    "    \"\"\"Calculate active weights vs benchmark for each position.\"\"\"\n",
    "    # Portfolio weights\n",
    "    total_value = df['market_value'].sum()\n",
    "    position_weights = df['market_value'] / total_value\n",
    "    \n",
    "    # Sector weights in portfolio\n",
    "    sector_totals = df.groupby('sector')['market_value'].transform('sum')\n",
    "    portfolio_sector_weights = sector_totals / total_value\n",
    "    \n",
    "    # Active weights (vs benchmark)\n",
    "    df = df.copy()\n",
    "    df['benchmark_sector_weight'] = df['sector'].map(benchmark_weights)\n",
    "    active_sector_weights = portfolio_sector_weights - df['benchmark_sector_weight']\n",
    "    \n",
    "    # Each position's contribution to active weight\n",
    "    position_sector_contribution = df['market_value'] / sector_totals\n",
    "    active_weight_contribution = position_sector_contribution * active_sector_weights\n",
    "    \n",
    "    return active_weight_contribution\n",
    "\n",
    "# Example benchmark weights for demonstration\n",
    "benchmark_weights = {\n",
    "    'Technology': 0.25, 'Healthcare': 0.15, 'Finance': 0.20,\n",
    "    'Energy': 0.08, 'Real Estate': 0.05, 'Consumer Goods': 0.12,\n",
    "    'Industrials': 0.08, 'Utilities': 0.04, 'Materials': 0.02, 'Telecom': 0.01\n",
    "}\n",
    "\n",
    "# Time the advanced calculations\n",
    "print(\"\\nð¦ ADVANCED FINANCIAL CALCULATIONS:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Risk Analytics\n",
    "start = time.time()\n",
    "sector_exposures = calculate_sector_exposures_optimized(portfolio)\n",
    "risk_time = time.time() - start\n",
    "print(f\"â Sector Exposure Analysis: {risk_time:.4f}s\")\n",
    "print(f\"  Sample exposure contributions: {sector_exposures.head().values}\")\n",
    "\n",
    "# Performance Attribution  \n",
    "start = time.time()\n",
    "factor_loadings = calculate_factor_loadings_optimized(portfolio)\n",
    "attribution_time = time.time() - start\n",
    "print(f\"â Factor Loading Calculation: {attribution_time:.4f}s\")\n",
    "print(f\"  Sample factor contributions: {factor_loadings.head().values}\")\n",
    "\n",
    "# Risk Management\n",
    "start = time.time()\n",
    "active_weights = calculate_active_weights_optimized(portfolio, benchmark_weights)\n",
    "risk_mgmt_time = time.time() - start\n",
    "print(f\"â Active Weight Calculation: {risk_mgmt_time:.4f}s\")\n",
    "print(f\"  Sample active weight contributions: {active_weights.head().values}\")\n",
    "\n",
    "print(f\"\\nð¡ Total time for all advanced calculations: {risk_time + attribution_time + risk_mgmt_time:.4f}s\")\n",
    "print(\"   These would take significantly longer with traditional apply() methods!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46804c3",
   "metadata": {},
   "source": [
    "## 6. Memory Usage Analysis\n",
    "\n",
    "Understanding memory consumption patterns is crucial for production systems. Let's analyze how both approaches use memory and identify the trade-offs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9e583b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in MB.\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / 1024 / 1024\n",
    "\n",
    "def analyze_memory_usage(n_rows=50000):\n",
    "    \"\"\"\n",
    "    Analyze memory usage patterns for both approaches.\n",
    "    \"\"\"\n",
    "    print(f\"Memory Analysis with {n_rows:,} rows\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Create test data\n",
    "    test_df = create_portfolio_data(n_rows)\n",
    "    baseline_memory = get_memory_usage()\n",
    "    print(f\"Baseline memory usage: {baseline_memory:.1f} MB\")\n",
    "    \n",
    "    # Test apply() approach memory usage\n",
    "    memory_before_apply = get_memory_usage()\n",
    "    apply_result = calculate_weighted_contributions_apply(test_df)\n",
    "    memory_after_apply = get_memory_usage()\n",
    "    apply_memory_delta = memory_after_apply - memory_before_apply\n",
    "    \n",
    "    # Clean up\n",
    "    del apply_result\n",
    "    \n",
    "    # Test transform() approach memory usage  \n",
    "    memory_before_transform = get_memory_usage()\n",
    "    transform_result = calculate_weighted_contributions_transform(test_df)\n",
    "    memory_after_transform = get_memory_usage()\n",
    "    transform_memory_delta = memory_after_transform - memory_before_transform\n",
    "    \n",
    "    print(f\"\\nMemory Usage Comparison:\")\n",
    "    print(f\"â¢ Apply() approach:     {apply_memory_delta:+.1f} MB\")\n",
    "    print(f\"â¢ Transform() approach: {transform_memory_delta:+.1f} MB\")\n",
    "    print(f\"â¢ Difference:           {transform_memory_delta - apply_memory_delta:+.1f} MB\")\n",
    "    \n",
    "    # Analyze DataFrame memory usage\n",
    "    print(f\"\\nDataFrame Memory Breakdown:\")\n",
    "    print(f\"â¢ Original DataFrame:   {test_df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "    print(f\"â¢ Result Series:        {transform_result.memory_usage(deep=True) / 1024**2:.1f} MB\")\n",
    "    \n",
    "    # Memory efficiency insights\n",
    "    original_size = test_df.memory_usage(deep=True).sum()\n",
    "    result_size = transform_result.memory_usage(deep=True)\n",
    "    efficiency_ratio = result_size / original_size\n",
    "    \n",
    "    print(f\"\\nð¡ Memory Efficiency Insights:\")\n",
    "    print(f\"â¢ Result size is {efficiency_ratio:.1%} of original DataFrame\")\n",
    "    print(f\"â¢ Transform creates temporary Series equal to DataFrame length\")\n",
    "    print(f\"â¢ Apply creates multiple smaller DataFrames (one per group)\")\n",
    "    \n",
    "    return {\n",
    "        'apply_memory_delta': apply_memory_delta,\n",
    "        'transform_memory_delta': transform_memory_delta,\n",
    "        'efficiency_ratio': efficiency_ratio\n",
    "    }\n",
    "\n",
    "# Run memory analysis\n",
    "memory_results = analyze_memory_usage(25000)\n",
    "\n",
    "# Memory usage recommendations\n",
    "print(f\"\\nð¯ MEMORY USAGE RECOMMENDATIONS:\")\n",
    "print(\"=\"*60)\n",
    "print(\"â Transform() is generally more memory-efficient for most use cases\")\n",
    "print(\"â Creates fewer temporary objects compared to apply()\")\n",
    "print(\"â Better cache locality due to vectorized operations\")\n",
    "print(\"â For extremely large datasets, consider chunked processing:\")\n",
    "\n",
    "print(\"\"\"\\n# Example chunked processing for very large datasets\n",
    "def process_large_dataset_chunked(file_path, chunk_size=100000):\n",
    "    results = []\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "        chunk_result = calculate_weighted_contributions_transform(chunk)\n",
    "        results.append(chunk_result)\n",
    "    return pd.concat(results)\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53e5917",
   "metadata": {},
   "source": [
    "## 7. When This Optimization Doesn't Work\n",
    "\n",
    "Not all group-wise calculations can be optimized with transform(). Let's explore scenarios where apply() is still necessary and understand the limitations of our optimization technique.\n",
    "\n",
    "**Transform() works when:**\n",
    "- Calculation can be decomposed into group aggregations + element-wise operations\n",
    "- No complex conditional logic within groups  \n",
    "- No row-by-row state management required\n",
    "\n",
    "**Apply() is still needed for:**\n",
    "- Complex conditional logic based on group characteristics\n",
    "- Operations requiring sorted data within groups\n",
    "- Calculations needing access to multiple group statistics simultaneously\n",
    "- Stateful operations that depend on previous calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363197be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples where apply() is still necessary\n",
    "\n",
    "def complex_risk_calculation_apply_only(group):\n",
    "    \"\"\"\n",
    "    Example: Complex risk calculation that can't be easily vectorized.\n",
    "    \n",
    "    This calculates a conditional risk metric based on:\n",
    "    - Portfolio concentration (if sector has >30% weight)\n",
    "    - Volatility ranking within the group\n",
    "    - Correlation-based adjustments\n",
    "    \"\"\"\n",
    "    if len(group) < 3:\n",
    "        # Not enough positions for meaningful calculation\n",
    "        return pd.Series([0.0] * len(group), index=group.index)\n",
    "    \n",
    "    # Sort by market value to get concentration ranking\n",
    "    sorted_group = group.sort_values('market_value', ascending=False)\n",
    "    \n",
    "    # Calculate concentration factor\n",
    "    total_value = group['market_value'].sum()\n",
    "    top_position_weight = sorted_group['market_value'].iloc[0] / total_value\n",
    "    \n",
    "    if top_position_weight > 0.3:\n",
    "        # High concentration penalty\n",
    "        concentration_factor = 1.5\n",
    "    elif top_position_weight > 0.2:\n",
    "        # Medium concentration\n",
    "        concentration_factor = 1.2\n",
    "    else:\n",
    "        # Well diversified\n",
    "        concentration_factor = 1.0\n",
    "    \n",
    "    # Volatility-based ranking (simulate volatility data)\n",
    "    group = group.copy()\n",
    "    group['volatility'] = np.random.uniform(0.1, 0.4, len(group))\n",
    "    group['vol_rank'] = group['volatility'].rank(pct=True)\n",
    "    \n",
    "    # Complex risk score calculation\n",
    "    risk_scores = []\n",
    "    for idx, row in group.iterrows():\n",
    "        base_risk = abs(row['monthly_return']) * row['volatility']\n",
    "        vol_adjustment = 1 + (row['vol_rank'] - 0.5) * 0.3  # Higher vol = higher risk\n",
    "        position_risk = base_risk * vol_adjustment * concentration_factor\n",
    "        risk_scores.append(position_risk)\n",
    "    \n",
    "    return pd.Series(risk_scores, index=group.index)\n",
    "\n",
    "def moving_average_calculation_apply_only(group):\n",
    "    \"\"\"\n",
    "    Example: Calculations requiring ordered data and state.\n",
    "    \n",
    "    Calculate 3-period moving average of returns within each group.\n",
    "    This requires ordered data and can't be easily vectorized.\n",
    "    \"\"\"\n",
    "    if len(group) < 3:\n",
    "        return pd.Series([np.nan] * len(group), index=group.index)\n",
    "    \n",
    "    # Sort by some date column (simulate with random order for demo)\n",
    "    sorted_group = group.sample(frac=1).reset_index()  # Random shuffle for demo\n",
    "    \n",
    "    # Calculate moving average\n",
    "    moving_avgs = []\n",
    "    for i in range(len(sorted_group)):\n",
    "        if i < 2:\n",
    "            moving_avgs.append(np.nan)  # Not enough data\n",
    "        else:\n",
    "            window_avg = sorted_group['monthly_return'].iloc[i-2:i+1].mean()\n",
    "            moving_avgs.append(window_avg)\n",
    "    \n",
    "    # Return with original index\n",
    "    result = pd.Series(moving_avgs, index=sorted_group['index'])\n",
    "    return result.reindex(group.index)\n",
    "\n",
    "# Test the complex calculations that require apply()\n",
    "print(\"ð« CALCULATIONS THAT REQUIRE APPLY():\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_portfolio = create_portfolio_data(1000)\n",
    "\n",
    "# Complex risk calculation\n",
    "print(\"1. Complex Risk Calculation (concentration + volatility)\")\n",
    "start = time.time()\n",
    "risk_scores = test_portfolio.groupby(['sector', 'region']).apply(complex_risk_calculation_apply_only)\n",
    "risk_time = time.time() - start\n",
    "print(f\"   Execution time: {risk_time:.4f}s\")\n",
    "print(f\"   Sample risk scores: {risk_scores.head().values}\")\n",
    "\n",
    "# Moving average calculation  \n",
    "print(\"\\n2. Moving Average Calculation (stateful/ordered)\")\n",
    "start = time.time()\n",
    "moving_avgs = test_portfolio.groupby('sector').apply(moving_average_calculation_apply_only)\n",
    "ma_time = time.time() - start\n",
    "print(f\"   Execution time: {ma_time:.4f}s\")\n",
    "print(f\"   Sample moving averages: {moving_avgs.dropna().head().values}\")\n",
    "\n",
    "print(f\"\\nâ ï¸  KEY LIMITATIONS OF TRANSFORM() OPTIMIZATION:\")\n",
    "print(\"=\"*70)\n",
    "print(\"â Cannot handle complex conditional logic based on group characteristics\")\n",
    "print(\"â Cannot work with calculations requiring specific data ordering\")\n",
    "print(\"â Cannot handle stateful calculations (moving averages, cumulative sums)\")\n",
    "print(\"â Cannot access multiple group statistics simultaneously in complex ways\")\n",
    "print(\"â Cannot handle calculations where result size differs from input size\")\n",
    "\n",
    "print(f\"\\nð¯ DECISION FRAMEWORK:\")\n",
    "print(\"=\"*30)\n",
    "print(\"â Use TRANSFORM() when: Simple aggregations + element-wise math\")\n",
    "print(\"â Use APPLY() when: Complex logic, state, ordering, or conditions\")\n",
    "print(\"â Always benchmark your specific use case!\")\n",
    "\n",
    "# Performance comparison for edge case: very few groups\n",
    "small_groups_df = create_portfolio_data(1000)\n",
    "small_groups_df['sector'] = np.random.choice(['A', 'B'], len(small_groups_df))  # Only 2 groups\n",
    "\n",
    "print(f\"\\nð EDGE CASE: Very Few Groups (2 groups, {len(small_groups_df)} rows)\")\n",
    "start = time.time()\n",
    "apply_small = calculate_weighted_contributions_apply(small_groups_df)\n",
    "apply_small_time = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "transform_small = calculate_weighted_contributions_transform(small_groups_df)\n",
    "transform_small_time = time.time() - start\n",
    "\n",
    "if apply_small_time > 0 and transform_small_time > 0:\n",
    "    small_speedup = apply_small_time / transform_small_time\n",
    "    print(f\"   Apply time: {apply_small_time:.6f}s\")\n",
    "    print(f\"   Transform time: {transform_small_time:.6f}s\") \n",
    "    print(f\"   Speedup: {small_speedup:.1f}x\")\n",
    "    if small_speedup < 2:\n",
    "        print(\"   ð¡ With very few groups, the speedup is minimal!\")\n",
    "else:\n",
    "    print(\"   Times too small to measure reliably\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99393d3",
   "metadata": {},
   "source": [
    "## 8. Summary and Key Takeaways\n",
    "\n",
    "This notebook demonstrated how `groupby().transform()` can dramatically improve the performance of group-wise calculations in financial data analysis. Here are the key insights:\n",
    "\n",
    "### ð Performance Gains\n",
    "- **Consistent speedups** of 5-50x across different dataset sizes\n",
    "- **Scales well** with larger datasets (better performance on bigger data)\n",
    "- **Identical results** to traditional `apply()` methods\n",
    "- **More memory efficient** for most use cases\n",
    "\n",
    "### ð¼ Financial Applications\n",
    "- **Portfolio Analytics**: Sector exposure, risk attribution\n",
    "- **Performance Attribution**: Factor loading calculations  \n",
    "- **Risk Management**: Active weights, concentration analysis\n",
    "- **Backtesting**: Historical performance metrics\n",
    "\n",
    "### âï¸ When to Use Each Method\n",
    "\n",
    "| Use Transform() When: | Use Apply() When: |\n",
    "|----------------------|-------------------|\n",
    "| Simple aggregations + math | Complex conditional logic |\n",
    "| Large datasets | Stateful calculations |\n",
    "| Performance is critical | Ordering matters |\n",
    "| Memory efficiency matters | Group-specific algorithms |\n",
    "\n",
    "### ð¯ Best Practices\n",
    "1. **Always verify** results match between methods\n",
    "2. **Benchmark** your specific use case\n",
    "3. **Profile memory usage** for large datasets\n",
    "4. **Consider chunked processing** for extremely large data\n",
    "5. **Document** your optimization choices for team members\n",
    "\n",
    "### ð Further Reading & Resources\n",
    "- [Pandas Performance Tips](https://pandas.pydata.org/docs/user_guide/enhancingperf.html)\n",
    "- [NumPy Broadcasting Rules](https://numpy.org/doc/stable/user/basics.broadcasting.html)\n",
    "- [Financial Data Processing Best Practices](https://github.com/pandas-dev/pandas/wiki/Pandas-Performance-Tips)\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to optimize your financial data pipelines?** Start by identifying your most time-consuming `groupby().apply()` operations and see if they can be refactored using the `transform()` approach!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
